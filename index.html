<h1>Week1作业
1.描述梯度下降，随机梯度下降和批量梯度下降的联系与区别。
联系：整体的方向都是向着全局最优解的。
区别：
批量梯度下降：梯度下降的每一步中都需要用到所有的训练样本，适合样本量比较小的情况。每次迭代得到的损失函数都向着全局最优方向，单峰问题上最终得到的是全局最优解，多峰问题最终得到的很有可能是局部最优解。
随机梯度下降：梯度下降的每一步中只随机用到一个样本，适合样本量比较大的情况。每次迭代得到的损失函数都是向着样本的最优方向，多峰情况更可能避免进入局部极小值中。

2.用sklean的线性模型完成kaggle房价预测问题。
照着链接代码在pycharm敲了一遍

import pandas as pd
import numpy as np
import matplotlib

import matplotlib.pyplot as plt
from scipy.stats import skew

train = pd.read_csv("../../data/week_1/train.csv")
test = pd.read_csv("../../data/week_1/test.csv")
print(train.head())

all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition']))
#print(all_data.head())

#指定绘图区域的大小
matplotlib.rcParams['figure.figsize'] = (12.0,6.0)
#对目标变量做对数处理
prices = pd.DataFrame({"price":train["SalePrice"],"log(price+1)":np.log1p(train["SalePrice"])})
prices.hist()
plt.show()
plt.clf()
plt.close()

train["SalePrice"] = np.log1p(train["SalePrice"])

#对数值型变量做对数处理
numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index
# print(numeric_feats)

#计算偏度系数：衡量了数据集的对称程度。偏度系数越接近0，这说明数据集越对称，越远离0则表明数据集越不对称。
# 如果偏度系数为正，则表明他的数据在右边更为分散，若为负的，则说明他的左侧更为分散
skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))
#print(skewed_feats)
skewed_feats = skewed_feats[skewed_feats > 0.75]
# print(skewed_feats)
skewed_feats = skewed_feats.index
# print(skewed_feats)

all_data[skewed_feats] = np.log1p(all_data[skewed_feats])
# print(all_data.head())

all_data = pd.get_dummies(all_data)
# print(all_data.head())

all_data = all_data.fillna(all_data.mean())
print(all_data.head())

X_train = all_data[:train.shape[0]]
X_test = all_data[train.shape[0]:]
y=train.SalePrice
print(y.head())


from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV
from sklearn.model_selection import cross_val_score

def rmse_cv(model):
    #交叉验证：cv参数用于规定将原始数据分成多少份
    rmse = np.sqrt(-cross_val_score(model, X_train, y, scoring="neg_mean_squared_error", cv=5))
    return rmse

model_ridge = Ridge()

alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]
cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]
print(cv_ridge)

cv_ridge = pd.Series(cv_ridge, index = alphas)
print(cv_ridge)
cv_ridge.plot(title = "Validation - Just Do It")
plt.xlabel("alpha")
plt.ylabel("rmse")
plt.show()
plt.clf()
plt.close()

print(cv_ridge.min())


model_lasso = LassoCV(alphas = [1, 0.1,0.001, 0.0005]).fit(X_train, y)
print(rmse_cv(model_lasso).mean())

coef = pd.Series(model_lasso.coef_,index=X_train.columns)
print(coef)

print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " + str(sum(coef == 0)) + " variables")

imp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])
print(imp_coef)

matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Lasso Model")
plt.show()
plt.clf()
plt.close()


matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)

preds = pd.DataFrame({"preds":model_lasso.predict(X_train), "true":y})
preds["residuals"] = preds["true"] - preds["preds"]
preds.plot(x = "preds", y = "residuals", kind = "scatter")
plt.show()
plt.clf()
plt.close()

import xgboost as xgb

dtrain = xgb.DMatrix(X_train,label=y)
dtest = xgb.DMatrix(X_test)

params = {"max_depth":2,"eta":0.1}

model = xgb.cv(params, dtrain,num_boost_round=500,early_stopping_rounds=100)
print(model.head())
model.loc[30:,["test-rmse-mean","train-rmse-mean"]].plot()
plt.show()
plt.clf()
plt.close()

model_xgb = xgb.XGBRegressor(n_estimators=360,max_depth=2,learning_rate=0.1)
model_xgb.fit(X_train,y)

xgb_preds = np.expm1(model_xgb.predict(X_test))
lasso_preds = np.expm1(model_lasso.predict(X_test))

predictions = pd.DataFrame({"xgb":xgb_preds,"lasso":lasso_preds})
predictions.plot(x="xgb",y="lasso", kind="scatter")
plt.show()
plt.clf()
plt.close()

preds = 0.7*lasso_preds + 0.3+xgb_preds

solution = pd.DataFrame({"id":test.Id,"SalePrice":preds})
solution.to_csv("../../data/week_1/ridge_sol.csv",index=False)

</h1>
